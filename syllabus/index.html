<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>syllabus | Natural Language Processing  CS 6120</title>
    <meta name="author" content="Natural Language Processing  CS 6120">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="nlp, ai, ml, natural-language, Northeastern, Computer Science, CS6120">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/cs6120f25/assets/css/main.css">
    <link rel="canonical" href="https://course.ccs.neu.edu//cs6120f25/syllabus/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/cs6120f25/assets/js/theme.js"></script>
    <script src="/cs6120f25/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/cs6120f25/"><span class="font-weight-bold">Natural Language Processing </span>CS 6120</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/cs6120f25/">home</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/cs6120f25/syllabus/">syllabus<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cs6120f25/submissions/">submissions</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cs6120f25/readings/">readings</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cs6120f25/staff/">personnel</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">previous classes</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="https://course.ccs.neu.edu/cs6120s25">spring 2025 (last semester)</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="https://course.ccs.neu.edu/cs6120f25">fall 2025 (this semester)</a>
                </div>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/cv.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">syllabus <a href="/cs6120f25/assets/pdf/nlp_syllabus.pdf" target="_blank" rel="noopener noreferrer" class="float-right"><i class="fas fa-file-pdf"></i></a>
</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <div class="cv">
              
                <div class="card mt-3 p-3">
                  <h3 class="card-title font-weight-medium">schedule</h3>
                  <div>
                  
                   <ul class="card-text font-weight-light list-group list-group-flush">
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 1
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1EvZAL5dpZK4B3GNFn6ASxG1K8ssyu0SW" rel="external nofollow noopener" target="_blank">Introduction and Applications</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">September 8 <a href="https://shorturl.at/VEzQr" rel="external nofollow noopener" target="_blank">(video)</a>
</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item"><b>Language</b> - the most efficient and compact way to transfer knowledge is through words, where the window to AGI is through NLP. This lecture is an introduction that takes us through history of how we got to LLMs. We'll also review some applications of NLP, current industry standards, and some of the most impactful approaches and where they are being implemented. Finally, we'll preview what we'll be learning, the logistics of how we'll be doing so, and the expectations for your participation in this class.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Applications Overview</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Machine Translation (Baidu's Word-Word)</span></li>
                      
                        <li><span class="subitem">Summarization (Dialogues, Newspaper Articles, etc.)</span></li>
                      
                        <li><span class="subitem">Text Classification and Clustering (News Article Groupings, etc.)</span></li>
                      
                        <li><span class="subitem">Question and Answering (LLMs and Chatbots)</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Submissions</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="../assets/pdf/lab-1.pdf">Laboratory - Getting Started on Google Cloud with Your Credits</a></span></li>
                      
                        <li><span class="subitem"><a href="../homework-1">Assignment 1 is assigned</a> - <i>A First Look at Processing Language</i></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 2
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/14PzCaOaPv4WEFvjR4IJ7o6RD-WSVr7ZW" rel="external nofollow noopener" target="_blank">ML Foundations and Software Engineering</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">September 15 <a href="https://shorturl.at/GZ8Oi" rel="external nofollow noopener" target="_blank">(video)</a>
</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">As NLP is a specific branch of machine learning, we will review some foundational knowledge that we'll utilize through the course of this class. We'll look at both machine learning and software engineering best practices that will help you build and scale NLP systems later in the course. Because most NLP algorithms today rely heavily on computing resources, we'll dive into distributed compution approaches and cloud-based operations.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Foundations of Machine Learning</span></li>
                      
                        <li><span class="subitem">Software Engineering Practices</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a></span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="../submissions">Submissions</a></span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="../assets/pdf/lab-2.pdf">Laboratory - Containerization in the Cloud</a></span></li>
                      
                        <li><span class="subitem"><a href="http://gradescope.com" rel="external nofollow noopener" target="_blank">Assignment 1 is <b>due</b></a></span></li>
                      
                        <li><span class="subitem"><a href="../homework-2">Assignment 2 is assigned</a> - <i>Text Classification</i></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 3
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1fE9hcD9UJq3GvPQczDxN8rX5_M3SED-8" rel="external nofollow noopener" target="_blank">Language Classification</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">September 22 <a href="https://shorturl.at/Aqq8N" rel="external nofollow noopener" target="_blank">(video)</a>
</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">Building upon our review of machine learning, we discuss strategies in feature extraction and generation. Particularly as creating a vocabulary can explode required memory space, our featurization includes NLP-specific techniques (e.g., tokenization, lemmatization, etc.). This week also marks the first week of <a href="../readings/"><b>required keynote paper</b></a> reading, where we will begin the tour of seminal papers that have revolutionized not only language processing but also machine learning and artificial intelligence writ large.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Building Vocabulary with Stopwords and Stemming</span></li>
                      
                        <li><span class="subitem">Preprocessing - Tokenization and Lemmatization</span></li>
                      
                        <li><span class="subitem">Logistic Regression Classifier</span></li>
                      
                        <li><span class="subitem">Naïve Bayes Classifiers</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><b>Application</b> - Sentiment Analysis</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="https://cdn.openai.com/gpt-5-system-card.pdf" rel="external nofollow noopener" target="_blank">Required Keynote Paper</a> - GPT-5 Model Card from OpenAI</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Older model reference - <a href="https://arxiv.org/abs/2303.08774" rel="external nofollow noopener" target="_blank"> OpenAI's GPT-4 technical report </a></span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Submissions</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/14wNBIyAOMTyh5Hnuv8tOBXdyV39H0QYW#" rel="external nofollow noopener" target="_blank">Laboratory - Naïve Bayes</a></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 4
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1x_xCsAgSqaO3fSjZyeKlBhMVH48syktb" rel="external nofollow noopener" target="_blank">Text Processing Algorithms</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">September 29 <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/EXTTL1-1fohPv8vJCLv32c0B6N0NFGQThTEToIULD9RdbA?e=yHd3pn" rel="external nofollow noopener" target="_blank">(video)</a>
</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">One of the most widely used algorithms in practice today are autocorrecting algorithms that typically have on-device requirements. In this lecture, we'll review elements of dynamic programming, particularly with respect to the minimum edit distance algorithm, and how we can apply these concepts to the autocorrect and subsequently the autocomplete problem.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Representations of Language</span></li>
                      
                        <li><span class="subitem">Comparisons / Differences in Language</span></li>
                      
                        <li><span class="subitem">Minimum Edit Distance Algorithms</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><b>Application</b> - Autocorrect in Practice</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="https://arxiv.org/abs/2503.23278" rel="external nofollow noopener" target="_blank">Required Keynote Paper</a> - Outrageously Large Neural Networks</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://arxiv.org/abs/2412.19437" rel="external nofollow noopener" target="_blank">DeepSeek Technical Report</a> - Mixture of Experts</span></li>
                      
                        <li><span class="subitem"><a href="https://huggingface.co/blog/moe" rel="external nofollow noopener" target="_blank">Hugging Face Blog</a></span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Submissions</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1M8i7DuIuUMKDHO3Kuu98ZE3xv60zupAV" rel="external nofollow noopener" target="_blank">Laboratory - Autocorrect Vocabulary Candidates</a></span></li>
                      
                        <li><span class="subitem"><a href="http://gradescope.com" rel="external nofollow noopener" target="_blank">Assignment 2 is <b>due</b></a></span></li>
                      
                        <li><span class="subitem"><a href="../homework-3">Assignment 3 is assigned</a> - <i>Autocorrect and Minimum Edit Distances</i></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 5
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/17hSy11npLcOLbgqHAMh0y4NLet3gSir9" rel="external nofollow noopener" target="_blank">Introduction to Language Modeling</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">October 6</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">Today we'll begin our journey to understanding LLMs by observing its origins. Dropping the "Large" from the now-ubiquitous term "Large Language Models", we take a look at the foundational principles that extract the relationships defining what it means to model language and how we might generate text.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">What is a language model? (Abstractive vs extractive approaches)</span></li>
                      
                        <li><span class="subitem">Overview of Basic Modeling Approaches</span></li>
                      
                        <li><span class="subitem">The N-Gram Model</span></li>
                      
                        <li><span class="subitem">Out of Vocabulary Words and Smoothing</span></li>
                      
                        <li><span class="subitem">Language Model Evaluation</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><b>Application</b> - Autocompleting words and sentences</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="https://arxiv.org/abs/2503.23278" rel="external nofollow noopener" target="_blank">Required Keynote Paper</a> - Model Context Protocol</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://www.anthropic.com/news/model-context-protocol" rel="external nofollow noopener" target="_blank">An Introduction to MCP</a> - Anthropic's Blog</span></li>
                      
                        <li><span class="subitem"><a href="https://modelcontextprotocol.io/introduction" rel="external nofollow noopener" target="_blank">MCP's Central Documentation</a></span></li>
                      
                        <li><span class="subitem"><a href="https://learn.deeplearning.ai/courses/mcp-build-rich-context-ai-apps-with-anthropic" rel="external nofollow noopener" target="_blank">DeepLearning.Net</a> - Anthropic's on MCP</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Submissions</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1AVRooWlPXLxNrREvGt_DQvfRZS6Q2_8b" rel="external nofollow noopener" target="_blank">Laboratory 5.1 - N-Grams Processing</a></span></li>
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1bSeH7jy3F8sgdPTDlYu8gdJW1IvuvucX" rel="external nofollow noopener" target="_blank">Laboratory 5.2 - Out of Vocabulary Words</a></span></li>
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1bSeH7jy3F8sgdPTDlYu8gdJW1IvuvucX" rel="external nofollow noopener" target="_blank">Laboratory 5.3 - Building the Language Model</a></span></li>
                      
                        <li><span class="subitem"><a href="http://gradescope.com" rel="external nofollow noopener" target="_blank">Assignment 3 is <b>due</b></a></span></li>
                      
                        <li><span class="subitem"><a href="../homework-4">Assignment 4 is assigned</a> - <i>Autocomplete with Topical Information</i></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 6
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4">Indigenous Peoples Day - Holiday</h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">October 13</h6>
            
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 7
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1kiEI4S3WLJ3cikQkZ85Nvx2Mz6kFyJLX" rel="external nofollow noopener" target="_blank">Unsupervised NLP - Topic Modeling</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">October 20</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">This week, we will explore David Blei's contributions to the field, a set of concepts that indirectly attack the age-old question of <b>"what is <i>k</i> in the k-means clustering algorithm</b>. We will review the hierarchical nature of how to model natural language using Bayesian concepts, where our corpora is processed without preserving the order of words. This week's keynote reading is perhaps the most difficult one that you'll read in this class, since it involves a heavy component of probability and statistics.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Parameter Estimation of a Distribution</span></li>
                      
                        <li><span class="subitem">The Dirichlet Distribution and its Attributes</span></li>
                      
                        <li><span class="subitem">Infinite Bayesian in Topic Models</span></li>
                      
                        <li><span class="subitem">Latent Semantic Indexing and Latent Dirichlet Allocation</span></li>
                      
                        <li><span class="subitem">(Collapsed) Gibbs Sampling, and Optimization</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><b>Application</b> - Grouping Documents</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="external nofollow noopener" target="_blank">Required Keynote Paper</a> - Latent Dirichlet Allocation</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://www.youtube.com/watch?v=FkckgwMHP2s" rel="external nofollow noopener" target="_blank">David Blei's Lecture</a></span></li>
                      
                        <li><span class="subitem"><a href="https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2" rel="external nofollow noopener" target="_blank">Introductory Blog to Topic Modeling</a></span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Submissions</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="../assets/pdf/lab-6.pdf">Laboratory - Jupyter Notebooks with GPUs</a></span></li>
                      
                        <li><span class="subitem"><a href="http://gradescope.com" rel="external nofollow noopener" target="_blank">Assignment 4 is <b>due</b></a></span></li>
                      
                        <li><span class="subitem"><a href="../homework-5">Assignment 5 is assigned</a></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 8
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1cnSHRYSq2esGADDQrVjTMzj9LhAAIfuy" rel="external nofollow noopener" target="_blank">Word Modeling with Self-Supervision</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">October 27</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">Perhaps the most influential paper to have come out of the natural language community is the <a href="https://arxiv.org/abs/1310.4546" rel="external nofollow noopener" target="_blank"><i>word2vec</i> paper</a> that most general machine learning practioners recognize. You'll find elements of its practice in communities from the information retrieval sciences to modern cyber applications to general ML problems. As it pertains to language models, modeling words is often the first stage in any system pipeline that you may design. This week's lecture reviews word models (including word2vec as well as continuous bags of words) and the embeddings / representations that they create.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Embeddings with Continuous Bag of Words</span></li>
                      
                        <li><span class="subitem">Intrinsic and Extrinsice Evaluation of Word Models</span></li>
                      
                        <li><span class="subitem">Word Modeling in Practice</span></li>
                      
                        <li><span class="subitem">The Skip-gram and Negative Sampling</span></li>
                      
                        <li><span class="subitem">From Words to Sentences</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="https://arxiv.org/abs/1310.4546" rel="external nofollow noopener" target="_blank">Required Keynote Paper</a> - Distributed Representations of Words</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://www.baeldung.com/cs/nlps-word2vec-negative-sampling" rel="external nofollow noopener" target="_blank">Blog - Gentle Introduction to Negative Sampling</a></span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Submissions</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1lknBFuNeVr7-AvQx4vKvCQfVkL_za65Z" rel="external nofollow noopener" target="_blank">Laboratory - Word Embeddings with CBOW</a></span></li>
                      
                        <li><span class="subitem"><a href="../assets/pdf/lab-7.pdf">Laboratory - The Original Word2Vec Code in C</a> (Optional)</span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 9
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1zmPWukoV4r1YYOqi6inWBAHTmoZQ6TA0" rel="external nofollow noopener" target="_blank">Introduction to Sequential Modeling</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">November 3</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">In smaller data regimes and for bespoke problems, we'll find that conversations still revolve around the OG state-driven modeling approach - Hidden Markov Models (HMMs). HMMs dominated NLP and speech recognition for half a century and are classic examples of NLP fundamentals. We'll build a strong foundation this week that appreciates the origins of our field, which helps us understand the motivations behind the latest Deep Learning more modern paradigms like Transformers.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Modeling with Hidden Markov Models</span></li>
                      
                        <li><span class="subitem">The Viterbi Algorithm - Initialization, Forward, and Backward Passes</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><b>Application</b> - Parts of Speech Tagging</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://arxiv.org/abs/2310.12321" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a> - A Survey of LLMs Including ChatGPT and GPT-4</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://arxiv.org/abs/2310.12321" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a> - Learning Text Similarity with Siamese Recurrent Networks</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Submissions</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1fAmkPaSkecIVhgljO4vAwz5dilDmodUc" rel="external nofollow noopener" target="_blank">Laboratory - HMMs Text Processing</a></span></li>
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/15TFAx6tERj2_K0BAhJ4_0nr2muBiyDUC" rel="external nofollow noopener" target="_blank">Laboratory - HMMs Numpy PoS Processing </a></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 10
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1U_CdfaLWgvswaw4T29p7oXDOrootGqmq" rel="external nofollow noopener" target="_blank">Recurrence and Neural Networks</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">November 10</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">While newer architectures like transformers now dominate the field of NLP, in its short tenure, Recurrent Neural Networks became workhorses that first demonstrated the power of deep learning for sequential data like text. This lecture builds an appreciation of how modeling language works, how attention and transformers originated, and subsequently the transition to truly deep architectures.  Beyond studying the history; it's we'll review the fundamental principles in RNNs that underpin modern NLP.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Traditional Language Models vs Recurrent Models</span></li>
                      
                        <li><span class="subitem">The Recurrent Neural Network</span></li>
                      
                        <li><span class="subitem">Vanishing and Exploding Gradients</span></li>
                      
                        <li><span class="subitem">Memory Gating - GRUs and LSTMs</span></li>
                      
                        <li><span class="subitem">Accuracy and Evaluation - Perplexity</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><b>Applications</b> - Named Entity Recognition and Machine Translation</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://deeplearning.cs.cmu.edu/F23/document/readings/LSTM.pdf" rel="external nofollow noopener" target="_blank"> Required Keynote Paper </a> Long Short Term Memory Networks</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="https://arxiv.org/abs/1211.5063" rel="external nofollow noopener" target="_blank"> Required Keynote Paper </a> - On the Difficulty of Training RNNs</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="external nofollow noopener" target="_blank"> Karpathy's Blog on Recurrent Networks</a></span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="../submissions">Submissions</a></span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1N2In_T8J1evlZPduXoxqeOpR4RE86AHK" rel="external nofollow noopener" target="_blank">Laboratory - Building Your First RNN</a></span></li>
                      
                        <li><span class="subitem"><a href="http://gradescope.com" rel="external nofollow noopener" target="_blank">Assignment 5 is <b>due</b></a></span></li>
                      
                        <li><span class="subitem"><a href="../homework-6">Assignment 6 is assigned</a> - <i>Implement Your Own Recurrent Network</i></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 11
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/14mvSG5KB7MHyyL_vTtVDCnwUiYC3_GDw" rel="external nofollow noopener" target="_blank">Attention and the Transformer Model</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">November 17</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">Attention models have been the leap forward that are the fundamental building blocks to modern machine learning today, including the essential ingredients for Large Language Models. We'll go deep into attention layers in neural networks, building our own from scratch.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Introduction to the Attention Modeling</span></li>
                      
                        <li><span class="subitem">The Self-Attention Mechanism</span></li>
                      
                        <li><span class="subitem">The Transformer Modeling Layer</span></li>
                      
                        <li><span class="subitem">Large Scale Attention Modeling</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a> - Attention is All You Need</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="https://arxiv.org/abs/1810.04805" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a> - BERT - Pre-training Bidirectional Transformers</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" rel="external nofollow noopener" target="_blank">BERT Explained - State of the art in NLP, Blog</a></span></li>
                      
                        <li><span class="subitem"><a href="https://www.youtube.com/watch?v=XowwKOAWYoQ&amp;themeRefresh=1" rel="external nofollow noopener" target="_blank">Attention Paper Explained</a></span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="../submissions">Submissions</a></span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/10SDgUL4px6M_R7-4c1EDHEx0neck2ZOq" rel="external nofollow noopener" target="_blank">Laboratory - Dot Product Attention</a></span></li>
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1j8tjxiN0G9ehF4VfihVqd2-m3QafDh1I" rel="external nofollow noopener" target="_blank">Laboratory - Masking in Attention</a></span></li>
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1L3WUB6fMTO4WEKnqPmdsqtMeiCkX2VCv" rel="external nofollow noopener" target="_blank">Laboratory - Positional Encoding</a></span></li>
                      
                        <li><span class="subitem"><a href="http://gradescope.com" rel="external nofollow noopener" target="_blank">Assignment 6 is <b>due</b></a></span></li>
                      
                        <li><span class="subitem"><a href="../homework-7">Assignment 7 is assigned</a> - <i>Attention and Transformer Networks</i></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 12
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1UCcxFqBzBp7-FCl957iyVKpetFNDZhU5EepNai_8-oI" rel="external nofollow noopener" target="_blank">Introduction to Large Language Modeling (LLMs)</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">November 24</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">The next two weeks are devoted to the state of the art in industry, and LLMs in practice, which may have changed in the time that you have started this course! This week, we introduce large language models using the fundamentals that you have learned, from perplexity in system design to transformer neural network layers for pre-training. We'll focus on techniques that large companies (or well-funded ones, at least) use to create <i>foundation</i> LLM models, taking training methods from OpenAI, Anthropic, Amazon, and Google.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Large Language Modeling (LLM) in Code</span></li>
                      
                        <li><span class="subitem">Aligning LLMs in the Instruction Following Framework</span></li>
                      
                        <li><span class="subitem">Tuning with Low Resources - LoRA and Quantization</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://arxiv.org/abs/2203.02155" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a> - Training to Instruct with Human Feedback</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://arxiv.org/abs/2303.08774" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a> - GPT-4 Technical Report from OpenAI</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="../submissions">Submissions</a></span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="../project-proposal/"> Project Proposals are <b>due</b></a></span></li>
                      
                        <li><span class="subitem"><a href="../assets/pdf/lab-11.1.pdf">Laboratory - Serving an LLM</a></span></li>
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1u8DDFEiCC2yJgQIfbCOY4_aN06MJeUfv" rel="external nofollow noopener" target="_blank">Laboratory - Tuning LLMs</a> (Optional)</span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 13
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/19SAm2OxY9ZvBU5fWQaVRg8G5dKvsNioj" rel="external nofollow noopener" target="_blank">Practically Leveraging LLMs and the LLM Lifecycle</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">December 1</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">Last week, we discussed how large companies might train LLMs. In contrast, this week's lecture is most useful for those interested in entering the industry at the mid- to startup levels, where we explore common approaches to optimally <i>leverage</i> large language models for your particular applications once the LLM has been created. These techniques additionally attack limitations in LLMs, such as knowledge gaps, hallucinations, and logical reasoning problems. Additionally, we explore the practical aspects of GenAI engineers when product managers ask them to design a system for them. More than the theory, we'll learn about the system itself, devoting time for *when* to focus on certain components of your LLM, and the life cycle of your system design.</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title">Lecturing Topics</span>
                      <ul class="subitems">
                      
                        <li><span class="subitem">Prompt Engineering - Query and Context</span></li>
                      
                        <li><span class="subitem">Retrieval Augmented Generation (RAG)</span></li>
                      
                        <li><span class="subitem">Tuning with Low Resources - LoRA and Quantization</span></li>
                      
                        <li><span class="subitem">Intelligent Agents with Program-Aided LLMs</span></li>
                      
                        <li><span class="subitem">Guidelines and NLP Systems Engineering Diagrams</span></li>
                      
                        <li><span class="subitem">Intelligent Agents with Program-Aided LLMs</span></li>
                      
                        <li><span class="subitem">Multimodal Large Language Models</span></li>
                      
                      </ul>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://arxiv.org/abs/2005.11401" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a> - Retrieval Augmented Generation</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item"><a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Required Keynote Reading</a> - Parameter Efficient Fine-Tuning</span>
                    
                  </li>
                
                  <li>
                    
                      <span class="item-title"><a href="../submissions">Submissions</a></span>
                      <ul class="subitems">
                      
                        <li><span class="subitem"><a href="https://colab.research.google.com/drive/1ogy2_MEcMEFrJGiS7XwtEElwnV7bY_p5" rel="external nofollow noopener" target="_blank">Laboratory - Instruction Following Tuning</a></span></li>
                      
                        <li><span class="subitem"><a href="http://gradescope.com" rel="external nofollow noopener" target="_blank">Assignment 7 is <b>due</b></a></span></li>
                      
                      </ul>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
      <li class="list-group-item">
        
        <div class="row">
          
            <div class="col-xs-2 cl-sm-2 col-md-2 text-center" style="width: 75px;">
              <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px;">
                Week 16
              </span>
            </div>
          
          <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0">
            
            <h6 class="title font-weight-bold ml-1 ml-md-4"><a href="https://docs.google.com/presentation/d/1VO-SAmfm3smmDhVn6AkyK-SZTDURDILz9xCFx_0OiAA" rel="external nofollow noopener" target="_blank">Demonstrations and Poster Sessions</a></h6>
            
            
            <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem;">December 8</h6>
            
            
              <ul class="items">
                
                  <li>
                    
                      <span class="item">Deploy and show off your domain-specific LLM and pitch your startup idea! Review the guidelines at the <a href="../presentation-and-project/">Final Project Website</a>.</span>
                    
                  </li>
                
              </ul>
            
            
          </div>
        </div>
      </li>
    
    </ul>

                  
                  </div>
                </div>
              
                <div class="card mt-3 p-3">
                  <h3 class="card-title font-weight-medium">grading criterion</h3>
                  <div>
                  
                    <table class="table table-sm table-borderless table-responsive">
    
      <tr>
        <td class="p-1 pr-2 font-weight-bold"><b>Participation</b></td>
        <td class="p-1 pl-2 font-weight-light text">5%</td>
      </tr>
    
      <tr>
        <td class="p-1 pr-2 font-weight-bold"><b>Reading Group</b></td>
        <td class="p-1 pl-2 font-weight-light text">15%</td>
      </tr>
    
      <tr>
        <td class="p-1 pr-2 font-weight-bold"><b>Labs</b></td>
        <td class="p-1 pl-2 font-weight-light text">25%</td>
      </tr>
    
      <tr>
        <td class="p-1 pr-2 font-weight-bold"><b>LLM Deployment Project</b></td>
        <td class="p-1 pl-2 font-weight-light text">25%</td>
      </tr>
    
      <tr>
        <td class="p-1 pr-2 font-weight-bold"><b>Assignments</b></td>
        <td class="p-1 pl-2 font-weight-light text">30%</td>
      </tr>
    
</table>
                  
                  </div>
                </div>
              
                <div class="card mt-3 p-3">
                  <h3 class="card-title font-weight-medium">grading policies</h3>
                  <div>
                  
                    <ul class="card-text font-weight-light list-group list-group-flush">
    
      <li class="list-group-item">Labs must be turned in by the following lecture</li>
    
      <li class="list-group-item">Late labs / assignments have 3pt deduction each weekday until the following lecture</li>
    
      <li class="list-group-item">Late assignments cannot be accepted past the time when the next homework assignment is released</li>
    
      <li class="list-group-item">You must notify cs6120-staff@ccs.neu.edu ahead of any absences.</li>
    
</ul>
                  
                  </div>
                </div>
              
                <div class="card mt-3 p-3">
                  <h3 class="card-title font-weight-medium">course meeting times</h3>
                  <div>
                  
                    <ul class="card-text font-weight-light list-group list-group-flush">
    
      <li class="list-group-item">
      <h5 class="font-italic">Lectures</h5>
      
        <ul class="subitems">
          
            <li><span class="subitem">Mon, 4pm-7:20pm</span></li>
          
            <li><span class="subitem">Room San Jose R1045</span></li>
          
        </ul>
      
      </li>
    
      <li class="list-group-item">
      <h5 class="font-italic">Office Hours</h5>
      
        <ul class="subitems">
          
            <li><span class="subitem">Karl, Thu 8:30-9:30pm, Teams</span></li>
          
            <li><span class="subitem">Dharun, Fri 12-2pm, 9th Floor</span></li>
          
            <li><span class="subitem">Swathi, Wed 12-2pm, 9th Floor</span></li>
          
            <li><span class="subitem">Qunnie, Thu 1-3pm, 10th Floor, Zoom</span></li>
          
        </ul>
      
      </li>
    
</ul>
                  
                  </div>
                </div>
              
                <div class="card mt-3 p-3">
                  <h3 class="card-title font-weight-medium">suggested textbooks</h3>
                  <div>
                  
                    <ul class="card-text font-weight-light list-group list-group-flush">
    
      <li class="list-group-item">
<a href="https://web.stanford.edu/~jurafsky/slp3/" rel="external nofollow noopener" target="_blank">Speech and Language Processing, 3rd Ed.</a> Dan Jurafsky and James Martin, 2024</li>
    
      <li class="list-group-item">
<a href="https://arxiv.org/pdf/2307.06435" rel="external nofollow noopener" target="_blank">A Comprehensive Overview of Large Language Models</a>, Naveed et. al., 2024</li>
    
</ul>
                  
                  </div>
                </div>
              
              </div>
          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © 2025 Natural Language Processing  CS 6120. Northeastern University, San Jose Campus, <a href="http://karllab41.github.io" rel="external nofollow noopener" target="_blank">Karl Ni</a>.  Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: September 30, 2025.
      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/cs6120f25/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/cs6120f25/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/cs6120f25/assets/js/common.js"></script>
  <script defer src="/cs6120f25/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
