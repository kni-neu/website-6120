- title: schedule
  type: time_table
  contents:
    - title: <a href="https://docs.google.com/presentation/d/1EvZAL5dpZK4B3GNFn6ASxG1K8ssyu0SW">Introduction and Applications</a>
      institution: September 8
      year: Week 1
      description:
        - <b>Language</b> - the most efficient and compact way to transfer knowledge is through words, where the window to AGI is through NLP. This lecture is an introduction that takes us through history of how we got to LLMs. We'll also review some applications of NLP, current industry standards, and some of the most impactful approaches and where they are being implemented. Finally, we'll preview what we'll be learning, the logistics of how we'll be doing so, and the expectations for your participation in this class.
        - title: Applications Overview
          contents:
            - Machine Translation (Baidu's Word-Word)
            - Summarization (Dialogues, Newspaper Articles, etc.)
            - Text Classification and Clustering (News Article Groupings, etc.)
            - Question and Answering (LLMs and Chatbots)
        - title: Submissions
          contents:
            - <a href="../assets/pdf/lab-1.pdf">Laboratory - Getting Started on Google Cloud with Your Credits</a>
            - <a href="../homework-1">Assignment 1 is assigned</a> - <i>A First Look at Processing Language</i>


    - title: <a href="https://docs.google.com/presentation/d/14PzCaOaPv4WEFvjR4IJ7o6RD-WSVr7ZW">ML Foundations and Software Engineering</a>
      institution: September 15
      year: Week 2
      description:
        - As NLP is a specific branch of machine learning, we will review some foundational knowledge that we'll utilize through the course of this class. We'll look at both machine learning and software engineering best practices that will help you build and scale NLP systems later in the course. Because most NLP algorithms today rely heavily on computing resources, we'll dive into distributed compution approaches and cloud-based operations.
        - title: Lecturing Topics
          contents:
            - Foundations of Machine Learning
            - Software Engineering Practices
        - <a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf">Required Keynote Reading</a>
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href="../assets/pdf/lab-2.pdf">Laboratory - Containerization in the Cloud</a>
            - <a href="http://gradescope.com">Assignment 1 is <b>due</b></a>
            - <a href="../homework-2">Assignment 2 is assigned</a> - <i>Text Classification</i>

            

    - title: <a href="https://docs.google.com/presentation/d/1fE9hcD9UJq3GvPQczDxN8rX5_M3SED-8">Language Classification</a>
      institution: September 22
      year: Week 3
      description:
        - Building upon our review of machine learning, we discuss strategies in feature extraction and generation. Particularly as creating a vocabulary can explode required memory space, our featurization includes NLP-specific techniques (e.g., tokenization, lemmatization, etc.). This week also marks the first week of <a href="../readings/"><b>required keynote paper</b></a> reading, where we will begin the tour of seminal papers that have revolutionized not only language processing but also machine learning and artificial intelligence writ large.
        - title: Lecturing Topics
          contents:
            - Building Vocabulary with Stopwords and Stemming
            - Preprocessing - Tokenization and Lemmatization
            - Logistic Regression Classifier
            - Naïve Bayes Classifiers
        - <b>Application</b> - Sentiment Analysis
        - title: <a href="https://cdn.openai.com/gpt-5-system-card.pdf">Required Keynote Paper</a> - GPT-5 Model Card from OpenAI
          contents:
            - Older model reference - <a href="https://arxiv.org/abs/2303.08774"> OpenAI's GPT-4 technical report </a>
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/14wNBIyAOMTyh5Hnuv8tOBXdyV39H0QYW#">Laboratory - Naïve Bayes</a>
            # - <a href="https://colab.research.google.com/drive/1JjgGfdbdjfVQ9uin35698sTn97xTeHeI">Laboratory - The NLTK Library</a>
            # - <a href="https://keras.io/examples/nlp/text_classification_from_scratch/">Laboratory - Python, PyTorch, Tensorflow, Colab, and Keras</a>



    - title: <a href="https://docs.google.com/presentation/d/1x_xCsAgSqaO3fSjZyeKlBhMVH48syktb">Text Processing Algorithms</a>
      institution: September 29
      year: Week 4
      description:
        - One of the most widely used algorithms in practice today are autocorrecting algorithms that typically have on-device requirements. In this lecture, we'll review elements of dynamic programming, particularly with respect to the minimum edit distance algorithm, and how we can apply these concepts to the autocorrect and subsequently the autocomplete problem.
        - title: Lecturing Topics
          contents:
            - Representations of Language
            - Comparisons / Differences in Language
            - Minimum Edit Distance Algorithms
        - <b>Application</b> - Autocorrect in Practice
        - title: <a href="https://arxiv.org/abs/2503.23278">Required Keynote Paper</a> - Outrageously Large Neural Networks
          contents:
            - <a href="https://arxiv.org/abs/2412.19437">DeepSeek Technical Report</a> - Mixture of Experts
            - <a href="https://huggingface.co/blog/moe">Hugging Face Blog</a>
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1M8i7DuIuUMKDHO3Kuu98ZE3xv60zupAV">Laboratory - Autocorrect Vocabulary Candidates</a>
            - <a href="http://gradescope.com">Assignment 2 is <b>due</b></a>
            - <a href="../homework-3">Assignment 3 is assigned</a> - <i>Autocorrect and Minimum Edit Distances</i>



    - title: <a href="https://docs.google.com/presentation/d/17hSy11npLcOLbgqHAMh0y4NLet3gSir9">Introduction to Language Modeling</a>
      institution: October 6
      year: Week 5
      description:
        - Today we'll begin our journey to understanding LLMs by observing its origins. Dropping the "Large" from the now-ubiquitous term "Large Language Models", we take a look at the foundational principles that extract the relationships defining what it means to model language and how we might generate text.  
        - title: Lecturing Topics
          contents:
            - What is a language model? (Abstractive vs extractive approaches)
            - Overview of Basic Modeling Approaches
            - The N-Gram Model
            - Out of Vocabulary Words and Smoothing
            - Language Model Evaluation
        - <b>Application</b> - Autocompleting words and sentences
        - title: <a href="https://arxiv.org/abs/2503.23278">Required Keynote Paper</a> - Model Context Protocol
          contents:
            - <a href="https://www.anthropic.com/news/model-context-protocol">An Introduction to MCP</a> - Anthropic's Blog
            - <a href="https://modelcontextprotocol.io/introduction">MCP's Central Documentation</a>
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1AVRooWlPXLxNrREvGt_DQvfRZS6Q2_8b">Laboratory 5.1 - N-Grams Processing</a>
            - <a href="https://colab.research.google.com/drive/1bSeH7jy3F8sgdPTDlYu8gdJW1IvuvucX">Laboratory 5.2 - Out of Vocabulary Words</a>
            - <a href="https://colab.research.google.com/drive/1bSeH7jy3F8sgdPTDlYu8gdJW1IvuvucX">Laboratory 5.3 - Building the Language Model</a>
            - <a href="http://gradescope.com">Assignment 3 is <b>due</b></a>
            - <a href="../homework-4">Assignment 4 is assigned</a> - <i>Autocomplete with Topical Information</i>


    - title: Indigenous Peoples Day - Holiday
      institution: October 13
      year: Week 6


    - title: <a href="https://docs.google.com/presentation/d/1kiEI4S3WLJ3cikQkZ85Nvx2Mz6kFyJLX">Unsupervised NLP - Topic Modeling</a>
      institution: October 20
      year: Week 7
      description:
        - This week, we will explore David Blei's contributions to the field, a set of concepts that indirectly attack the age-old question of <b>"what is <i>k</i> in the k-means clustering algorithm</b>. We will review the hierarchical nature of how to model natural language using Bayesian concepts, where our corpora is processed without preserving the order of words. This week's keynote reading is perhaps the most difficult one that you'll read in this class, since it involves a heavy component of probability and statistics. 
        - title: Lecturing Topics
          contents:
            - Parameter Estimation of a Distribution
            - The Dirichlet Distribution and its Attributes
            - Infinite Bayesian in Topic Models
            - Latent Semantic Indexing and Latent Dirichlet Allocation
            - (Collapsed) Gibbs Sampling, and Optimization
        - <b>Application</b> - Grouping Documents
        - title: <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Required Keynote Paper</a> - Latent Dirichlet Allocation
          contents:
            - <a href="https://www.youtube.com/watch?v=FkckgwMHP2s">David Blei's Lecture</a>
            - <a href="https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2">Introductory Blog to Topic Modeling</a>
        - title: Submissions
          contents:
            - <a href="../assets/pdf/lab-6.pdf">Laboratory - Jupyter Notebooks with GPUs</a>
            - <a href="http://gradescope.com">Assignment 4 is <b>due</b></a>
            - <a href="../homework-5">Assignment 5 is assigned</a> 
            # - <a href="https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/Introduction%20to%20topic%20modeling.ipynb">Laboratory</a> - Topic Modeling



    - title: <a href="https://docs.google.com/presentation/d/1cnSHRYSq2esGADDQrVjTMzj9LhAAIfuy">Word Modeling with Self-Supervision</a>
      institution: October 27
      year: Week 8
      description:
        - Perhaps the most influential paper to have come out of the natural language community is the <a href="https://arxiv.org/abs/1310.4546"><i>word2vec</i> paper</a> that most general machine learning practioners recognize. You'll find elements of its practice in communities from the information retrieval sciences to modern cyber applications to general ML problems. As it pertains to language models, modeling words is often the first stage in any system pipeline that you may design. This week's lecture reviews word models (including word2vec as well as continuous bags of words) and the embeddings / representations that they create.
        - title: Lecturing Topics
          contents: 
            - Embeddings with Continuous Bag of Words
            - Intrinsic and Extrinsice Evaluation of Word Models
            - Word Modeling in Practice
            - The Skip-gram and Negative Sampling
            - From Words to Sentences
        - title: <a href="https://arxiv.org/abs/1310.4546">Required Keynote Paper</a> - Distributed Representations of Words
          contents:
            - <a href="https://www.baeldung.com/cs/nlps-word2vec-negative-sampling">Blog - Gentle Introduction to Negative Sampling</a>
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1lknBFuNeVr7-AvQx4vKvCQfVkL_za65Z">Laboratory - Word Embeddings with CBOW</a>
            - <a href="../assets/pdf/lab-7.pdf">Laboratory - The Original Word2Vec Code in C</a> (Optional)


    - title: <a href="https://docs.google.com/presentation/d/1zmPWukoV4r1YYOqi6inWBAHTmoZQ6TA0">Introduction to Sequential Modeling</a>
      institution: November 3
      year: Week 9
      description:
        - In smaller data regimes and for bespoke problems, we'll find that conversations still revolve around the OG state-driven modeling approach - Hidden Markov Models (HMMs). HMMs dominated NLP and speech recognition for half a century and are classic examples of NLP fundamentals. We'll build a strong foundation this week that appreciates the origins of our field, which helps us understand the motivations behind the latest Deep Learning more modern paradigms like Transformers.
        - title: Lecturing Topics
          contents:
            - Modeling with Hidden Markov Models
            - The Viterbi Algorithm - Initialization, Forward, and Backward Passes
        - <b>Application</b> - Parts of Speech Tagging
        - <a href="https://arxiv.org/abs/2310.12321">Required Keynote Reading</a> - A Survey of LLMs Including ChatGPT and GPT-4
        - <a href="https://arxiv.org/abs/2310.12321">Required Keynote Reading</a> - Learning Text Similarity with Siamese Recurrent Networks
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1fAmkPaSkecIVhgljO4vAwz5dilDmodUc">Laboratory - HMMs Text Processing</a>
            - <a href="https://colab.research.google.com/drive/15TFAx6tERj2_K0BAhJ4_0nr2muBiyDUC">Laboratory - HMMs Numpy PoS Processing </a>
        

    - title: <a href="https://docs.google.com/presentation/d/1U_CdfaLWgvswaw4T29p7oXDOrootGqmq">Recurrence and Neural Networks</a>
      institution: November 10
      year: Week 10
      description:
        - While newer architectures like transformers now dominate the field of NLP, in its short tenure, Recurrent Neural Networks became workhorses that first demonstrated the power of deep learning for sequential data like text. This lecture builds an appreciation of how modeling language works, how attention and transformers originated, and subsequently the transition to truly deep architectures.  Beyond studying the history; it's we'll review the fundamental principles in RNNs that underpin modern NLP. 
        - title: Lecturing Topics
          contents:
            - Traditional Language Models vs Recurrent Models
            - The Recurrent Neural Network
            - Vanishing and Exploding Gradients
            - Memory Gating - GRUs and LSTMs
            - Accuracy and Evaluation - Perplexity
        - <b>Applications</b> - Named Entity Recognition and Machine Translation
        - <a href="https://deeplearning.cs.cmu.edu/F23/document/readings/LSTM.pdf"> Required Keynote Paper </a> Long Short Term Memory Networks
        - title: <a href="https://arxiv.org/abs/1211.5063"> Required Keynote Paper </a> - On the Difficulty of Training RNNs
          contents:
            - <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"> Karpathy's Blog on Recurrent Networks</a>
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href = "https://colab.research.google.com/drive/1N2In_T8J1evlZPduXoxqeOpR4RE86AHK">Laboratory - Building Your First RNN</a>
            - <a href="http://gradescope.com">Assignment 5 is <b>due</b></a>
            - <a href="../homework-6">Assignment 6 is assigned</a> - <i>Implement Your Own Recurrent Network</i>
          

    - title: <a href="https://docs.google.com/presentation/d/14mvSG5KB7MHyyL_vTtVDCnwUiYC3_GDw">Attention and the Transformer Model</a>
      institution: November 17
      year: Week 11
      description:
        - Attention models have been the leap forward that are the fundamental building blocks to modern machine learning today, including the essential ingredients for Large Language Models. We'll go deep into attention layers in neural networks, building our own from scratch.
        - title: Lecturing Topics
          contents:
            - Introduction to the Attention Modeling
            - The Self-Attention Mechanism
            - The Transformer Modeling Layer
            - Large Scale Attention Modeling
        - <a href="https://arxiv.org/abs/1706.03762">Required Keynote Reading</a> - Attention is All You Need
        - title: <a href="https://arxiv.org/abs/1810.04805">Required Keynote Reading</a> - BERT - Pre-training Bidirectional Transformers
          contents:
            - <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT Explained - State of the art in NLP, Blog</a>
            - <a href="https://www.youtube.com/watch?v=XowwKOAWYoQ&themeRefresh=1">Attention Paper Explained</a>
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href = "https://colab.research.google.com/drive/10SDgUL4px6M_R7-4c1EDHEx0neck2ZOq">Laboratory - Dot Product Attention</a>
            - <a href = "https://colab.research.google.com/drive/1j8tjxiN0G9ehF4VfihVqd2-m3QafDh1I">Laboratory - Masking in Attention</a>
            - <a href = "https://colab.research.google.com/drive/1L3WUB6fMTO4WEKnqPmdsqtMeiCkX2VCv">Laboratory - Positional Encoding</a>
            - <a href="http://gradescope.com">Assignment 6 is <b>due</b></a>
            - <a href="../homework-7">Assignment 7 is assigned</a> - <i>Attention and Transformer Networks</i>


    - title: <a href="https://docs.google.com/presentation/d/1UCcxFqBzBp7-FCl957iyVKpetFNDZhU5EepNai_8-oI">Introduction to Large Language Modeling (LLMs)</a>
      institution: November 24
      year: Week 12
      description:
        - The next two weeks are devoted to the state of the art in industry, and LLMs in practice, which may have changed in the time that you have started this course! This week, we introduce large language models using the fundamentals that you have learned, from perplexity in system design to transformer neural network layers for pre-training. We'll focus on techniques that large companies (or well-funded ones, at least) use to create <i>foundation</i> LLM models, taking training methods from OpenAI, Anthropic, Amazon, and Google.
        - title: Lecturing Topics
          contents:
            - Large Language Modeling (LLM) in Code
            - Aligning LLMs in the Instruction Following Framework
            - Tuning with Low Resources - LoRA and Quantization
        - <a href="https://arxiv.org/abs/2203.02155">Required Keynote Reading</a> - Training to Instruct with Human Feedback
        - <a href="https://arxiv.org/abs/2303.08774">Required Keynote Reading</a> - GPT-4 Technical Report from OpenAI
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href="../project-proposal/"> Project Proposals are <b>due</b></a>
            - <a href="../assets/pdf/lab-11.1.pdf">Laboratory - Serving an LLM</a>
            - <a href="https://colab.research.google.com/drive/1u8DDFEiCC2yJgQIfbCOY4_aN06MJeUfv">Laboratory - Tuning LLMs</a> (Optional)


    - title: <a href="https://docs.google.com/presentation/d/19SAm2OxY9ZvBU5fWQaVRg8G5dKvsNioj">Practically Leveraging LLMs and the LLM Lifecycle</a>
      institution: December 1
      year: Week 13
      description:
        - Last week, we discussed how large companies might train LLMs. In contrast, this week's lecture is most useful for those interested in entering the industry at the mid- to startup levels, where we explore common approaches to optimally <i>leverage</i> large language models for your particular applications once the LLM has been created. These techniques additionally attack limitations in LLMs, such as knowledge gaps, hallucinations, and logical reasoning problems. Additionally, we explore the practical aspects of GenAI engineers when product managers ask them to design a system for them. More than the theory, we'll learn about the system itself, devoting time for *when* to focus on certain components of your LLM, and the life cycle of your system design. 
        - title: Lecturing Topics
          contents:
            - Prompt Engineering - Query and Context
            - Retrieval Augmented Generation (RAG)
            - Tuning with Low Resources - LoRA and Quantization
            - Intelligent Agents with Program-Aided LLMs
            - Guidelines and NLP Systems Engineering Diagrams
            - Intelligent Agents with Program-Aided LLMs
            - Multimodal Large Language Models
        - <a href="https://arxiv.org/abs/2005.11401">Required Keynote Reading</a> - Retrieval Augmented Generation
        - <a href="https://arxiv.org/abs/1706.03762">Required Keynote Reading</a> - Parameter Efficient Fine-Tuning        
        - title: <a href="../submissions">Submissions</a>
          contents:
          - <a href="https://colab.research.google.com/drive/1ogy2_MEcMEFrJGiS7XwtEElwnV7bY_p5">Laboratory - Instruction Following Tuning</a>
          - <a href="http://gradescope.com">Assignment 7 is <b>due</b></a>


    - title: <a href="https://docs.google.com/presentation/d/1VO-SAmfm3smmDhVn6AkyK-SZTDURDILz9xCFx_0OiAA">Demonstrations and Poster Sessions</a>
      institution: December 8
      year: Week 16
      description:
        - Deploy and show off your domain-specific LLM and pitch your startup idea! Review the guidelines at the <a href="../presentation-and-project/">Final Project Website</a>.



- title: grading criterion
  type: map
  contents:
    - name: Participation
      value: 5%
    - name: Reading Group
      value: 15%
    - name: Labs
      value: 25%
    - name: LLM Deployment Project
      value: 25%
    - name: Assignments
      value: 30%

- title: grading policies
  type: list
  contents: 
    - Labs must be turned in by the following lecture
    - Late labs / assignments have 3pt deduction each weekday until the following lecture
    - Late assignments cannot be accepted past the time when the next homework assignment is released
    - You must notify cs6120-staff@ccs.neu.edu ahead of any absences.


- title: course meeting times
  type: nested_list
  contents:
    - title: Lectures
      items: 
        - "Mon, 4pm-7:20pm"
        - Room San Jose R1045
    - title: Office Hours
      items:
        - "Karl, Tues 8:30-9:30pm, Teams"
        - "Dharun, Fri 1-3pm, 9th Floor"
        - "Swathi, Wed 12-2pm, 9th Floor"
        - "Qunnie, Thu 1-3pm, 10th Floor, Zoom" 

- title: suggested textbooks
  type: list
  contents:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing, 3rd Ed.</a> Dan Jurafsky and James Martin, 2024
    - <a href="https://arxiv.org/pdf/2307.06435">A Comprehensive Overview of Large Language Models</a>, Naveed et. al., 2024
