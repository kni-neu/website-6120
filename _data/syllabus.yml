- title: schedule
  type: time_table
  contents:
    - title: <a href="https://docs.google.com/presentation/d/1Jg37kz_8CVGiEa4OMt6Dnb9F2giyUjQX">Introduction and Applications</a>
      institution: January 12 
      year: Week 1 (<a href="https://shorturl.at/1h1co">video</a>)
      description:
        - <b>Language</b> - the most efficient and compact way to transfer knowledge is through words, where the window to AGI is through NLP. This lecture is an introduction that takes us through history of how we got to LLMs. We'll also review some applications of NLP, current industry standards, and some of the most impactful approaches and where they are being implemented. Finally, we'll preview what we'll be learning, the logistics of how we'll be doing so, and the expectations for your participation in this class.
        - title: Applications Overview
          contents:
            - Machine Translation (Baidu's Word-Word)
            - Summarization (Dialogues, Newspaper Articles, etc.)
            - Text Classification and Clustering (News Article Groupings, etc.)
            - Question and Answering (LLMs and Chatbots)
        - title: Submissions
          contents:
            - <a href="../assets/pdf/lab-1.pdf">Laboratory - Getting Started on Google Cloud with Your Credits</a>
            - <a href="../homework-1">Assignment 1 is assigned</a> - <i>A First Look at Processing Language</i>

    - title: Martin Luther King, Jr. Day
      institution: January 19
      year: Week 2


    - title: <a href="https://docs.google.com/presentation/d/12XMsenosJHJoC5VZOQrbmXq0pG7Ove0y">ML Foundations and Software Engineering</a>
      institution: January 26
      year: Week 3
      description:
        - As NLP is a specific branch of machine learning, we will review some foundational knowledge that we'll utilize through the course of this class. We'll look at both machine learning and software engineering best practices that will help you build and scale NLP systems later in the course. Because most NLP algorithms today rely heavily on computing resources, we'll dive into distributed compution approaches and cloud-based operations.
        - title: Lecturing Topics
          contents:
            - Foundations of Machine Learning
            - Software Engineering Practices
        - <a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf">Required Keynote Reading</a> Linear Algebra and Gradients
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href="../assets/pdf/lab-2.pdf">Laboratory - Containerization in the Cloud</a>
            - <a href="http://gradescope.com">Assignment 1 is <b>due</b></a>
            - <a href="../homework-2">Assignment 2 is assigned</a> - <i>Text Classification</i>

            

    - title: <a href="https://docs.google.com/presentation/d/13olQ4iYGLOgAbYSP-l1uxeypHMTfdKFL">Language Classification</a>
      institution: February 2
      year: Week 4
      description:
        - Building upon our review of machine learning, we discuss strategies in feature extraction and generation. Particularly as creating a vocabulary can explode required memory space, our featurization includes NLP-specific techniques (e.g., tokenization, lemmatization, etc.). This week also marks the first week of <a href="../readings/"><b>required keynote paper</b></a> reading, where we will begin the tour of seminal papers that have revolutionized not only language processing but also machine learning and artificial intelligence writ large.
        - title: Lecturing Topics
          contents:
            - Building Vocabulary with Stopwords and Stemming
            - Preprocessing - Tokenization and Lemmatization
            - Logistic Regression Classifier
            - Naïve Bayes Classifiers
        - <b>Application</b> - Sentiment Analysis
        - <a href="https://arxiv.org/abs/2511.16043">Required Keynote Paper</a> - Agentic Learning without Data
        - <a href="https://arxiv.org/abs/1704.04760">Required Keynote Paper</a> -  Tensor Processing Unit Analysis
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/14wNBIyAOMTyh5Hnuv8tOBXdyV39H0QYW#">Laboratory - Naïve Bayes</a>
            # - <a href="https://colab.research.google.com/drive/1JjgGfdbdjfVQ9uin35698sTn97xTeHeI">Laboratory - The NLTK Library</a>
            # - <a href="https://keras.io/examples/nlp/text_classification_from_scratch/">Laboratory - Python, PyTorch, Tensorflow, Colab, and Keras</a>



    - title: <a href="https://docs.google.com/presentation/d/1NaaiOkdFXizWyrBv9lYZHUMPb3waDygW">Text Processing Algorithms</a>
      institution: February 9
      year: Week 5
      description:
        - One of the most widely used algorithms in practice today are autocorrecting algorithms that typically have on-device requirements. In this lecture, we'll review elements of dynamic programming, particularly with respect to the minimum edit distance algorithm, and how we can apply these concepts to the autocorrect and subsequently the autocomplete problem.
        - title: Lecturing Topics
          contents:
            - Representations of Language
            - Comparisons / Differences in Language
            - Minimum Edit Distance Algorithms
        - <b>Application</b> - Autocorrect in Practice
        - <a href="https://arxiv.org/abs/2503.23278">Required Keynote Paper</a> - Agent to Application Protocols
        - <a href="https://arxiv.org/abs/2505.02279">Required Keynote Paper</a> - Agent Interoperability Protocols
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1M8i7DuIuUMKDHO3Kuu98ZE3xv60zupAV">Laboratory - Autocorrect Vocabulary Candidates</a>
            - <a href="http://gradescope.com">Assignment 2 is <b>due</b></a>
            - <a href="../homework-3">Assignment 3 is assigned</a> - <i>Autocorrect and Minimum Edit Distances</i>

    - title: President's Day - No Class
      institution: February 16
      year: Week 6

    - title: <a href="https://docs.google.com/presentation/d/1Bil62cMryeAIHDX9CcWZqbakle6Or3b3">Introduction to Language Modeling</a>
      institution: February 23
      year: Week 7
      description:
        - Today we'll begin our journey to understanding LLMs by observing its origins. Dropping the "Large" from the now-ubiquitous term "Large Language Models", we take a look at the foundational principles that extract the relationships defining what it means to model language and how we might generate text.  
        - title: Lecturing Topics
          contents:
            - What is a language model? (Abstractive vs extractive approaches)
            - Overview of Basic Modeling Approaches
            - The N-Gram Model
            - Out of Vocabulary Words and Smoothing
            - Language Model Evaluation
        - <b>Application</b> - Autocompleting words and sentences
        - <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Required Keynote Paper</a> - Topic Modeling with Latent Dirichlet Allocation
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1AVRooWlPXLxNrREvGt_DQvfRZS6Q2_8b">Laboratory 5.1 - N-Grams Processing</a>
            - <a href="https://colab.research.google.com/drive/1bSeH7jy3F8sgdPTDlYu8gdJW1IvuvucX">Laboratory 5.2 - Out of Vocabulary Words</a>
            - <a href="https://colab.research.google.com/drive/1Ul6HAglGBoc53yAIfui-pgeTso7wMB26">Laboratory 5.3 - Building the Language Model</a>
            - <a href="http://gradescope.com">Assignment 3 is <b>due</b></a>
            - <a href="../homework-4">Assignment 4 is assigned</a> - <i>Autocomplete with Topical Information</i>


    - title: Enjoy Your Spring Break!
      institution: March 2
      year: Week 8


    - title: <a href="https://docs.google.com/presentation/d/1kesXmMT5rdxPMdeQ1EnKvcfwk8Z8vJez_">Word Modeling with Self-Supervision</a>
      institution: March 9
      year: Week 9
      description:
        - Perhaps the most influential paper to have come out of the natural language community is the <a href="https://arxiv.org/abs/1310.4546"><i>word2vec</i> paper</a> that most general machine learning practioners recognize. You'll find elements of its practice in communities from the information retrieval sciences to modern cyber applications to general ML problems. As it pertains to language models, modeling words is often the first stage in any system pipeline that you may design. This week's lecture reviews word models (including word2vec as well as continuous bags of words) and the embeddings / representations that they create.
        - title: Lecturing Topics
          contents: 
            - Embeddings with Continuous Bag of Words
            - Intrinsic and Extrinsice Evaluation of Word Models
            - Word Modeling in Practice
            - The Skip-gram and Negative Sampling
            - From Words to Sentences
        - <a href="https://arxiv.org/abs/2310.12321">Required Keynote Reading</a> - Learning Text Similarity with Siamese Recurrent Networks
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1lknBFuNeVr7-AvQx4vKvCQfVkL_za65Z">Laboratory - Word Embeddings with CBOW</a>
            - <a href="../assets/pdf/lab-7.pdf">Laboratory - The Original Word2Vec Code in C</a> (Optional)
            - <a href="http://gradescope.com">Assignment 4 is <b>due</b></a>
            - <a href="../homework-5">Assignment 5 is assigned</a> - <i>Word Embeddings and Their Applications</i>


    - title: <a href="https://docs.google.com/presentation/d/1zmPWukoV4r1YYOqi6inWBAHTmoZQ6TA0">Introduction to Sequential Modeling</a>
      institution: March 16
      year: Week 10
      description:
        - Sequence relates to the order by which we see information, and it separates modern NLP from statistical methods. This week, we will learn about the modeling of sequences from soup to nuts, and how that translates to the every day transformer that exists in almost every LLM today. To get there, we will investigate smaller data regimes and for bespoke problems, where some applications still leverage OG state-driven modeling approach - Hidden Markov Models (HMMs). Then, we'll move onto recurrent neural networks, which in its short tenure as the de facto default NLP algorithm,  formed the foundation of neural models and paved the way for modern neural networks.
        - title: Lecturing Topics
          contents:
            - Modeling with Hidden Markov Models
            - The Viterbi Algorithm - Initialization, Forward, and Backward Passes
            - The Recurrent Neural Network
            - Vanishing and Exploding Gradients
            - Memory Gating - GRUs and LSTMs
        - <b>Application</b> - Parts of Speech Tagging, Named Entity Recognition, and Machine Translation
        - title: <a href="https://arxiv.org/abs/1310.4546">Required Keynote Paper</a> - Distributed Representations of Words
          contents:
            - <a href="https://www.baeldung.com/cs/nlps-word2vec-negative-sampling">Blog - Gentle Introduction to Negative Sampling</a>
        - <a href="https://arxiv.org/abs/2512.02556"> Required Keynote Paper</a> - DeepSeek v3.2 Technical Report
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href="https://colab.research.google.com/drive/1fAmkPaSkecIVhgljO4vAwz5dilDmodUc">Laboratory - HMMs Text Processing</a>
            - <a href="https://colab.research.google.com/drive/15TFAx6tERj2_K0BAhJ4_0nr2muBiyDUC">Laboratory - HMMs Numpy PoS Processing </a>
            - <a href = "https://colab.research.google.com/drive/1N2In_T8J1evlZPduXoxqeOpR4RE86AHK">Laboratory - Building Your First RNN</a>
            - <a href="http://gradescope.com">Assignment 5 is <b>due</b></a>
            - <a href="../homework-6">Assignment 6 is assigned</a> - <i>Implement Your Own Recurrent Network</i>
          

    - title: <a href="https://docs.google.com/presentation/d/1L2UfukJVr2f6Lf3A6v3OIaPp9upmjqdP">Attention and the Transformer Model</a>
      institution: March 23
      year: Week 11
      description:
        - Attention models have been the leap forward that are the fundamental building blocks to modern machine learning today, including the essential ingredients for Large Language Models. We'll go deep into attention layers in neural networks, building our own from scratch.
        - title: Lecturing Topics
          contents:
            - Introduction to the Attention Modeling
            - The Self-Attention Mechanism
            - The Transformer Modeling Layer
            - Large Scale Attention Modeling
        - title: <a href="https://arxiv.org/abs/1211.5063"> Required Keynote Paper </a> - On the Difficulty of Training RNNs
          contents:
            - <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"> Karpathy's Blog on Recurrent Networks</a>
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href = "https://colab.research.google.com/drive/10SDgUL4px6M_R7-4c1EDHEx0neck2ZOq">Laboratory - Dot Product Attention</a>
            - <a href = "https://colab.research.google.com/drive/1j8tjxiN0G9ehF4VfihVqd2-m3QafDh1I">Laboratory - Masking in Attention</a>
            - <a href = "https://colab.research.google.com/drive/1L3WUB6fMTO4WEKnqPmdsqtMeiCkX2VCv">Laboratory - Positional Encoding</a>
            - <a href="http://gradescope.com">Assignment 6 is <b>due</b></a>
            - <a href="../homework-7">Assignment 7 is assigned</a> - <i>Attention and Transformer Networks</i>


    - title: <a href="https://docs.google.com/presentation/d/1nMyC1w6ItzDjDw3xXLJeRN0DOyZu1QiAh9lDMe-mn3A">Introduction to Large Language Modeling (LLMs)</a>
      institution: March 30
      year: Week 12
      description:
        - The next two weeks are devoted to the state of the art in industry, and LLMs in practice, which may have changed in the time that you have started this course! This week, we introduce large language models using the fundamentals that you have learned, from perplexity in system design to transformer neural network layers for pre-training. We'll focus on techniques that large companies (or well-funded ones, at least) use to create <i>foundation</i> LLM models, taking training methods from OpenAI, Anthropic, Amazon, and Google.
        - title: Lecturing Topics
          contents:
            - Large Language Modeling (LLM) in Code
            - Aligning LLMs in the Instruction Following Framework
            - Tuning with Low Resources - LoRA and Quantization
        - <a href="https://arxiv.org/abs/1706.03762">Required Keynote Reading</a> - Attention is All You Need
        - title: <a href="https://arxiv.org/abs/1810.04805">Required Keynote Reading</a> - BERT - Pre-training Bidirectional Transformers
          contents:
            - <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT Explained - State of the art in NLP, Blog</a>
            - <a href="https://www.youtube.com/watch?v=XowwKOAWYoQ&themeRefresh=1">Attention Paper Explained</a>
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href="../project-proposal/"> Project Proposals are <b>due</b></a>
            - <a href="../assets/pdf/lab-11.1.pdf">Laboratory - Serving an LLM</a>
            - <a href="https://colab.research.google.com/drive/1u8DDFEiCC2yJgQIfbCOY4_aN06MJeUfv">Laboratory - Tuning LLMs</a> (Optional)
            - <a href="http://gradescope.com">Assignment 7 is <b>due</b></a>


    - title: <a href="https://docs.google.com/presentation/d/1Kh1rLlQ3rAK4BuRPPyWXG5d_Ba_39n-_">Practically Leveraging LLMs and the LLM Lifecycle</a>
      institution: April 6
      year: Week 13
      description:
        - Last week, we discussed how large companies might train LLMs. In contrast, this week's lecture is most useful for those interested in entering the industry at the mid- to startup levels, where we explore common approaches to optimally <i>leverage</i> large language models for your particular applications once the LLM has been created. These techniques additionally attack limitations in LLMs, such as knowledge gaps, hallucinations, and logical reasoning problems. Additionally, we explore the practical aspects of GenAI engineers when product managers ask them to design a system for them. More than the theory, we'll learn about the system itself, devoting time for *when* to focus on certain components of your LLM, and the life cycle of your system design. 
        - title: Lecturing Topics
          contents:
            - Prompt Engineering - Query and Context
            - Retrieval Augmented Generation (RAG)
            - Tuning with Low Resources - LoRA and Quantization
            - Intelligent Agents with Program-Aided LLMs
            - Guidelines and NLP Systems Engineering Diagrams
            - Intelligent Agents with Program-Aided LLMs
            - Multimodal Large Language Models
        - <a href="https://arxiv.org/abs/2203.02155">Required Keynote Reading</a> - Training to Instruct with Human Feedback
        - <a href="https://arxiv.org/abs/2005.11401">Required Keynote Reading</a> - Retrieval Augmented Generation
        - title: <a href="../submissions">Submissions</a>
          contents:
          - <a href="https://colab.research.google.com/drive/1ogy2_MEcMEFrJGiS7XwtEElwnV7bY_p5">Laboratory - Instruction Following Tuning</a>


    - title: <a href="https://docs.google.com/presentation/d/1VO-SAmfm3smmDhVn6AkyK-SZTDURDILz9xCFx_0OiAA">Demonstrations and Poster Sessions</a>
      institution: April 13
      year: Week 14
      description:
        - Deploy and show off your domain-specific LLM and pitch your startup idea! Review the guidelines at the <a href="../presentation-and-project/">Final Project Website</a>.
        - <a href="https://arxiv.org/abs/1706.03762">Required Keynote Reading</a> - Low Rank Approximations for DNN Tuning

- title: grading criterion
  type: map
  contents:
    - name: Participation
      value: 10%
    - name: Labs
      value: 10%
    - name: Reading Group
      value: 20%
    - name: LLM Deployment Project
      value: 20%
    - name: Quizzes
      value: 20%
    - name: Assignments
      value: 20%

- title: grading policies
  type: list
  contents: 
    - Labs must be turned in by the following lecture
    - Late labs / assignments have 3pt deduction each weekday until the following lecture
    - Late assignments cannot be accepted past the time when the next homework assignment is released
    - You must notify cs6120-staff@ccs.neu.edu ahead of any absences.

- title: course meeting times
  type: nested_list
  contents:
    - title: Lectures
      items: 
        - "Mon, 4pm-7:20pm"
        - Room San Jose R1045
    - title: Office Hours
      items:
        - Karl, Thu 8:30-9:30pm, <a href="https://shorturl.at/Clp29">Teams</a>
        - Alwin, Tue 3-5pm, 10th Floor
        - Swathi, Wed 12-2pm, 9th Floor
        - Qunnie, Thu 1-3pm, 10th Floor, Zoom

- title: suggested textbooks
  type: list
  contents:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing, 3rd Ed.</a> Dan Jurafsky and James Martin, 2024
    - <a href="https://arxiv.org/pdf/2307.06435">A Comprehensive Overview of Large Language Models</a>, Naveed et. al., 2024
