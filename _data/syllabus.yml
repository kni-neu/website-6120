- title: schedule
  type: time_table
  contents:
    - title: <a href="https://docs.google.com/presentation/d/1eBrC_240_8mZiZzLAFR7zuizs75GnS7F">Introduction and Applications</a>
      institution: January 9 <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/EQvIPBEkwhFBkgyAQn-u6OoBe-YN92zyDFnfEoG33clTyg?e=eFSIXp">(video - part 1)</a>, <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/EVHeNhOBBc9CmaZVVvqsNNcB1CZiBBP84d0xZZgcSITiDA?e=ahn2ag">(video - part 2)</a>
      year: Week 1
      description:
        - <b>Language</b> - the most efficient and compact way to transfer knowledge is through words, where the window to AGI is through NLP. This lecture is an introduction that takes us through history of how we got to LLMs. We'll also review some applications of NLP, current industry standards, and some of the most impactful approaches and where they are being implemented. Finally, we'll preview what we'll be learning, the logistics of how we'll be doing so, and the expectations for your participation in this class.
        - title: Applications Overview
          contents:
            - Machine Translation (Baidu's Word-Word)
            - Summarization (Dialogues, Newspaper Articles, etc.)
            - Text Classification and Clustering (News Article Groupings, etc.)
            - Question and Answering (LLMs and Chatbots)
        - title: Submissions
          contents:
            - <a href="../assets/pdf/lab-1.pdf">Laboratory - Getting Started on Google Cloud with Your Credits</a>
            - <a href="../homework-1">Assignment 1 is assigned</a> - <i>A First Look at Processing Language</i>


    - title: <a href="https://docs.google.com/presentation/d/1PPcKkjG44GnkpF20HgVCyWCWeXgmpcXW">ML Foundations and Software Engineering</a>
      institution: January 16 <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/EXUov4aUUYhGiIhhYLkrSU8BN8d6WDBACQkFO2UAvpUtqg?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D&e=buUiuA"> (video) </a>
      year: Week 2
      description:
        - As NLP is a specific branch of machine learning, we will review some foundational knowledge that we'll utilize through the course of this class. We'll look at both machine learning and software engineering best practices that will help you build and scale NLP systems later in the course. Because most NLP algorithms today rely heavily on computing resources, we'll dive into distributed compution approaches and cloud-based operations.
        - title: Lecturing Topics
          contents:
            - Foundations of Machine Learning
            - Software Engineering Practices
        - <a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf">Required Keynote Reading</a>
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href="../assets/pdf/lab-2.pdf">Laboratory - Containerization in the Cloud</a>
            - <a href="http://gradescope.com">Assignment 1 is <b>due</b></a>
            - <a href="../homework-2">Assignment 2 is assigned</a> - <i>Text Classification</i>

            

    - title: <a href="https://docs.google.com/presentation/d/15xw5pKPLVPJYA5MnWtFvX0ljpmZ9vO8O">Language Classification</a>
      institution: January 23 <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/EbzVgjjnwQJMlXOzfSt1uBYBikUZhJktFXIfcbBzuSv4xQ?e=5Udz4N">(video)</a>
      year: Week 3
      description:
        - Building upon our review of machine learning, we discuss strategies in feature extraction and generation. Particularly as creating a vocabulary can explode required memory space, our featurization includes NLP-specific techniques (e.g., tokenization, lemmatization, etc.).
        - title: Lecturing Topics
          contents:
            - Building Vocabulary with Stopwords and Stemming
            - Preprocessing - Tokenization and Lemmatization
            - Logistic Regression Classifier
            - Naïve Bayes Classifiers
        - <b>Application</b> - Sentiment Analysis
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/14wNBIyAOMTyh5Hnuv8tOBXdyV39H0QYW#">Laboratory - Naïve Bayes</a>
            # - <a href="https://colab.research.google.com/drive/1JjgGfdbdjfVQ9uin35698sTn97xTeHeI">Laboratory - The NLTK Library</a>
            # - <a href="https://keras.io/examples/nlp/text_classification_from_scratch/">Laboratory - Python, PyTorch, Tensorflow, Colab, and Keras</a>



    - title: <a href="https://docs.google.com/presentation/d/1Hshg9q3zPhPXZoUTGbJXNnImx5XQJbnP">Text Processing Algorithms</a>
      institution: January 30 <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/ESEgcWCpovtInq8NBiZX5CkBIu7pzyC4eBQpgmZSEiFUMA?e=7OGaf7">(video)</a>
      year: Week 4
      description:
        - One of the most widely used algorithms in practice today are autocorrecting algorithms that typically have on-device requirements. In this lecture, we'll review elements of dynamic programming, particularly with respect to the minimum edit distance algorithm, and how we can apply these concepts to the autocorrect and subsequently the autocomplete problem.
        - title: Lecturing Topics
          contents:
            - Representations of Language
            - Comparisons / Differences in Language
            - Minimum Edit Distance Algorithms
        - <b>Application</b> - Autocorrect in Practice
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1M8i7DuIuUMKDHO3Kuu98ZE3xv60zupAV">Laboratory - Autocorrect Vocabulary Candidates</a>
            - <a href="http://gradescope.com">Assignment 2 is <b>due</b></a>
            - <a href="../homework-3">Assignment 3 is assigned</a> - <i>Autocorrect and Minimum Edit Distances</i>



    - title: <a href="https://docs.google.com/presentation/d/1csrZOD846CFWIHUkTiclrN2z2143_DSU">Introduction to Language Modeling</a>
      institution: February 6
      year: Week 5
      description:
        - title: Lecturing Topics
          contents:
            - What is a language model? (Abstractive vs extractive approaches)
            - Overview of Basic Modeling Approaches
            - The N-Gram Model
            - Out of Vocabulary Words and Smoothing
            - Language Model Evaluation
        - <b>Application</b> - Autocompleting words and sentences
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1AVRooWlPXLxNrREvGt_DQvfRZS6Q2_8b">Laboratory 5.1 - N-Grams Processing</a>
            - <a href="https://colab.research.google.com/drive/1bSeH7jy3F8sgdPTDlYu8gdJW1IvuvucX">Laboratory 5.2 - Out of Vocabulary Words</a>
            - <a href="https://colab.research.google.com/drive/1bSeH7jy3F8sgdPTDlYu8gdJW1IvuvucX">Laboratory 5.3 - Building the Language Model</a>
            - <a href="http://gradescope.com">Assignment 3 is <b>due</b></a>
            - <a href="../homework-4">Assignment 4 is assigned</a> - <i>Autocomplete with Topical Information</i>



    - title: <a href="https://docs.google.com/presentation/d/1WYYE4ShD1TE9KvT4ETJW_KMnhxXYZ5GS">Unsupervised NLP - Topic Modeling</a>
      institution: February 13
      year: Week 6
      description:
        - This week, we will explore David Blei's contributions to the field, a set of concepts that indirectly attack the age-old question of <b>"what is <i>k</i> in the k-means clustering algorithm</b>. We will review the hierarchical nature of how to model natural language using Bayesian concepts, where our corpora is processed without preserving the order of words. This week also marks the first week of <a href="../readings/"><b>required keynote paper</b></a> reading, where we will begin the tour of seminal papers that have revolutionized not only language processing but also machine learning and artificial intelligence writ large. This reading is perhaps the most difficult one that you'll read in this class, since it involves a heavy component of probability and statistics. 
        - title: Lecturing Topics
          contents:
            - Parameter Estimation of a Distribution
            - The Dirichlet Distribution and its Attributes
            - Infinite Bayesian in Topic Models
            - Latent Semantic Indexing and Latent Dirichlet Allocation
            - (Collapsed) Gibbs Sampling, and Optimization
        - <b>Application</b> - Grouping Documents
        - title: <a href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Required Keynote Paper</a> - Latent Dirichlet Allocation
          contents:
            - <a href="https://www.youtube.com/watch?v=FkckgwMHP2s">David Blei's Lecture</a>
            - <a href="https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2">Introductory Blog to Topic Modeling</a>
        - title: Submissions
          contents:
            - <a href="../assets/pdf/lab-6.pdf">Laboratory - Jupyter Notebooks with GPUs</a>
            # - <a href="https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/Introduction%20to%20topic%20modeling.ipynb">Laboratory</a> - Topic Modeling



    - title: <a href="https://docs.google.com/presentation/d/1rYrFIvU3UvYzJUNQwBYX6ZS_W1OtGM7L">Word Modeling with Self-Supervision</a>
      institution: February 20 <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/EVpfJu8GgI1LpOrVtaV0kMoB_aTiVoXMIjd5Sr9eQBdXMA?e=YgKS5q">(video)</a>
      year: Week 7
      description:
        - Perhaps the most influential paper to have come out of the natural language community is the <a href="https://arxiv.org/abs/1310.4546"><i>word2vec</i> paper</a> that most general machine learning practioners recognize. You'll find elements of its practice in communities from the information retrieval sciences to modern cyber applications to general ML problems. As it pertains to language models, modeling words is often the first stage in any system pipeline that you may design. This week's lecture reviews word models (including word2vec as well as continuous bags of words) and the embeddings / representations that they create.
        - title: Lecturing Topics
          contents: 
            - Embeddings with Continuous Bag of Words
            - Intrinsic and Extrinsice Evaluation of Word Models
            - Word Modeling in Practice
            - The Skip-gram and Negative Sampling
            - From Words to Sentences

        - title: <a href="https://arxiv.org/abs/1310.4546">Required Keynote Paper</a> - Distributed Representations of Words
          contents:
            - <a href="https://www.baeldung.com/cs/nlps-word2vec-negative-sampling">Blog - Gentle Introduction to Negative Sampling</a>
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1lknBFuNeVr7-AvQx4vKvCQfVkL_za65Z">Laboratory - Word Embeddings with CBOW</a>
            - <a href="../assets/pdf/lab-7.pdf">Laboratory - The Original Word2Vec Code in C</a> (Optional)
            - <a href="http://gradescope.com">Assignment 4 is <b>due</b></a>
            - <a href="../homework-5">Assignment 5 is assigned</a> - <i>Word2Vec - Skipgram Implementation (Optional)</i>



    - title: <a href="https://docs.google.com/presentation/d/1GdtnBqL5T2yHoj203MWOERBLUDM0hKiQ">Introduction to Sequential Modeling</a>
      institution: February 27 <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/Eb4yJiDxXB5FnL1NaRTWLpgBCuWasFLXUDD2tLUhhGeRSA?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D&e=nWNfBd">(video)</a>
      year: Week 8
      description:
        - title: Topics
          contents:
            - Modeling with Hidden Markov Models
            - The Viterbi Algorithm - Initialization, Forward, and Backward Passes
        - <b>Application</b> - Parts of Speech Tagging
        - <a href="https://arxiv.org/abs/2310.12321">Required Keynote Reading</a> - A Survey of LLMs Including ChatGPT and GPT-4
        - <a href="https://arxiv.org/abs/2310.12321">Required Keynote Reading</a> - Learning Text Similarity with Siamese Recurrent Networks
        - title: Submissions
          contents:
            - <a href="https://colab.research.google.com/drive/1fAmkPaSkecIVhgljO4vAwz5dilDmodUc">Laboratory - HMMs Text Processing</a>
            - <a href="https://colab.research.google.com/drive/15TFAx6tERj2_K0BAhJ4_0nr2muBiyDUC">Laboratory - HMMs Numpy PoS Processing </a>
        

    - title: No Instruction - Spring Break
      institution: March 6
      year: Week 9
      description:
        - Have a nice holiday!


    - title: <a href="https://docs.google.com/presentation/d/14AEBfimE6MUv85VRf9nNkWpcuQBqcurj">Recurrence and Neural Networks</a>
      institution: March 13 <a href="https://northeastern-my.sharepoint.com/:v:/g/personal/k_ni_northeastern_edu/ESm5C6Xub15OivT2pcDn0nQB1J3-nulJwQdj3YwuJcB1TA?nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D&e=i0EsUt">(video)</a>
      year: Week 10
      description:
        - While newer architectures like transformers now dominate the field of NLP, in its short tenure, Recurrent Neural Networks became workhorses that first demonstrated the power of deep learning for sequential data like text. This lecture builds an appreciation of how modeling language works, how attention and transformers originated, and subsequently the transition to truly deep architectures.  Beyond studying the history; it's we'll review the fundamental principles in RNNs that underpin modern NLP. 
        - title: Lecturing Topics
          contents:
            - Traditional Language Models vs Recurrent Models
            - The Recurrent Neural Network
            - Vanishing and Exploding Gradients
            - Memory Gating - GRUs and LSTMs
            - Accuracy and Evaluation - Perplexity
        - <b>Applications</b> - Named Entity Recognition and Machine Translation
        - <a href="https://deeplearning.cs.cmu.edu/F23/document/readings/LSTM.pdf"> Required Keynote Paper </a> Long Short Term Memory Networks
        - title: <a href="https://arxiv.org/abs/1211.5063"> Required Keynote Paper </a> - On the Difficulty of Training RNNs
          contents:
            - <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/"> Karpathy's Blog on Recurrent Networks</a>
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href = "https://colab.research.google.com/drive/1N2In_T8J1evlZPduXoxqeOpR4RE86AHK">Laboratory - Building Your First RNN</a>
            - <a href="http://gradescope.com">Assignment 5 is <b>due</b></a>
            - <a href="../homework-6">Assignment 6 is assigned</a> - <i>Implement Your Own Recurrent Network</i>


    # - title: <a href="https://docs.google.com/presentation/d/1xuEopbmKPls3tePdSf6VlHUMrGh-BAsc">Siamese Networks and Embedding Hashing <b>(No Lecture)</b></a>
    #   institution: March 20
    #   year: Week 11
    #   description:
    #     - title: <b>No Lecture</b> - Review these Topics
    #       contents:
    #         - Siamese Architectures and Shared Parameter Networks
    #         - The Triplet Data Objective
    #         - Transforming and Retrieving Vectors
    #         - Locality Sensitive Hashing, Quantization, and Approximate Nearest Neighbors
    #     - <b>Application</b> - Sentence Comparisons and Document Retrieval
    #     - title: <a href="https://aclanthology.org/W16-1617.pdf">Required Keynote Paper</a> - Text Similarity with Siamese RNNs
    #       contents:
    #         - <a href="https://www.youtube.com/watch?v=6jfw8MuKwpI">Video - Andrew Ng's Siamese Networks (for Images)</a>
    #     - title: <a href="../submissions">Submissions</a>
    #       contents:
    #       - <a href="http://gradescope.com">Assignment 6 is <b>due</b></a>
    #       - <a href="../homework-7">Assignment 7 is assigned</a> - <i>Coding Up Attention Networks</i>
          

    - title: <a href="https://docs.google.com/presentation/d/1v9Z5e0pqL2uIr2VdQ9rSN9_ff_OPYL_5">Attention and the Transformer Model</a>
      institution: March 20
      year: Week 11
      description:
        - Attention models have been the leap forward that are the fundamental building blocks to modern machine learning today, including the essential ingredients for Large Language Models. We'll go deep into attention layers in neural networks, building our own from scratch.
        - title: Lecturing Topics
          contents:
            - Introduction to the Attention Modeling
            - The Self-Attention Mechanism
            - The Transformer Modeling Layer
            - Large Scale Attention Modeling
        - <a href="https://arxiv.org/abs/1706.03762">Required Keynote Reading</a> - Attention is All You Need
        - title: <a href="https://arxiv.org/abs/1810.04805">Required Keynote Reading</a> - BERT - Pre-training Bidirectional Transformers
          contents:
            - <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT Explained - State of the art in NLP, Blog</a>
            - <a href="https://www.youtube.com/watch?v=XowwKOAWYoQ&themeRefresh=1">Attention Paper Explained</a>
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href = "https://colab.research.google.com/drive/10SDgUL4px6M_R7-4c1EDHEx0neck2ZOq">Laboratory - Dot Product Attention</a>
            - <a href = "https://colab.research.google.com/drive/1j8tjxiN0G9ehF4VfihVqd2-m3QafDh1I">Laboratory - Masking in Attention</a>
            - <a href = "https://colab.research.google.com/drive/1L3WUB6fMTO4WEKnqPmdsqtMeiCkX2VCv">Laboratory - Positional Encoding</a>
            - <a href="http://gradescope.com">Assignment 6 is <b>due</b></a>
            - <a href="../homework-6">Assignment 7 is assigned</a> - <i>Attention and Transformer Networks</i>



    - title: <a href="https://docs.google.com/presentation/d/1s-jVE_cQJ5fyEyPCIzHRu34sGrH4D9Mcrn0QGsOOmKA">Introduction to Large Language Modeling (LLMs)</a>
      institution: March 27
      year: Week 12
      description:
        - The next three weeks are devoted to the state of the art in industry, and LLMs in practice, which may have changed in the time that you have started this course! This week, we introduce large language models using the fundamentals that you have learned, from perplexity in system design to transformer neural network layers for pre-training. We'll focus on techniques that large companies (or well-funded ones, at least) use to create <i>foundation</i> LLM models, taking training methods from OpenAI, Anthropic, Amazon, and Google.
        - title: Lecturing Topics
          contents:
            - Large Language Modeling (LLM) in Code
            - Tuning with Low Resources - LoRA and Quantization
        - <a href="https://arxiv.org/abs/2203.02155">Required Keynote Reading</a> - Training to Instruct with Human Feedback
        - <a href="https://arxiv.org/abs/2303.08774">Required Keynote Reading</a> - GPT-4 Technical Report from OpenAI
        - title: <a href="../submissions">Submissions</a>
          contents:
            - <a href="../project-proposal/"> Project Proposals are <b>due</b></a>
            - <a href="../assets/pdf/lab-11.1.pdf">Laboratory - Serving an LLM</a>
            - <a href="https://colab.research.google.com/drive/1u8DDFEiCC2yJgQIfbCOY4_aN06MJeUfv">Laboratory - Tuning LLMs (Optional)</a>


    - title: <a href="https://docs.google.com/presentation/d/1ALNDyLPm7qaQFEwEW_JWGNwzZZdfb6Cc">Practically Leveraging Large Language Models</a>
      institution: April 3
      year: Week 13
      description:
        - Last week, we discussed how large companies might train LLMs. In contrast, this week's lecture is most useful for those interested in entering the industry at the mid- to startup levels, where we explore common approaches to optimally <i>leverage</i> large language models for your particular applications once the LLM has been created. These techniques additionally attack limitations in LLMs, such as knowledge gaps, hallucinations, and logical reasoning problems.
        - title: Lecturing Topics
          contents:
            - Prompt Engineering - In Context Learning
            - Aligning LLMs in the Instruction Following Framework
            - Deep Reinforcement Learning from Human Feedback
            - Retrieval Augmented Generation (RAG)
        - <a href="https://arxiv.org/abs/2005.11401">Required Keynote Reading</a> - Retrieval Augmented Generation
        - <a href="https://arxiv.org/abs/1706.03762">Required Keynote Reading</a> - Parameter Efficient Fine-Tuning        
        - title: <a href="../submissions">Submissions</a>
          contents:
          - <a href="https://colab.research.google.com/drive/1ogy2_MEcMEFrJGiS7XwtEElwnV7bY_p5">Laboratory - Instruction Following Tuning</a>
          - <a href="http://gradescope.com">Assignment 7 is <b>due</b></a>


    - title: Lecturer on Travel (No Lecture)
      institution: April 10
      year: Week 14


    - title: <a href="https://docs.google.com/presentation/d/13YrStTyouGcsJtHK4vUtIkCbC5UgPjD8YqDHwxGxI5Y">Language Modeling Systems Lifecycle</a>
      institution: April 17
      year: Week 15
      description:
        - You've learned about the inner workings of the LLM, the mechanisms that power it, how and when to tune it, the data collection processes that govern it, how it can be used practically, and the agents that can run on it. In this lecture, we explore the practical aspects of GenAI engineers when product managers ask them to design a system for them. More than the theory, we'll learn about the system itself, devoting time for *when* to focus on certain components of your LLM, and the life cycle of your system design. 
        - title: Lecturing Topics
          contents:
            - Guidelines and NLP Systems Engineering Diagrams
            - Intelligent Agents with Program-Aided LLMs
            - Multimodal Large Language Models
        - <b>Applications</b> - Creating Your Own GenAI Smart Agents


    - title: <a href="https://docs.google.com/presentation/d/1VO-SAmfm3smmDhVn6AkyK-SZTDURDILz9xCFx_0OiAA">Demonstrations and Poster Sessions</a>
      institution: April 24
      year: Week 16
      description:
        - Deploy and show off your domain-specific LLM and pitch your startup idea! Review the guidelines at the <a href="../presentation-and-project/">Final Project Website</a>.



- title: grading criterion
  type: map
  contents:
    - name: Participation
      value: 5%
    - name: Reading Group
      value: 15%
    - name: Labs
      value: 25%
    - name: LLM Deployment Project
      value: 25%
    - name: Assignments
      value: 30%


- title: course meeting times
  type: nested_list
  contents:
    - title: Lectures
      items: 
        - "Thurs, 4pm-7:20pm"
        - Room 1045
    - title: Office Hours
      items:
        - "Karl, Tues 8:30-9:30pm"
        - "Raman, Mon 1-3pm, 9th Floor"
        - "Joy, Tues 12-2pm, 9th Floor"
        - "Bella, Wed 1-3pm, 9th Floor" 

- title: suggested textbooks
  type: list
  contents:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing, 3rd Ed.</a> Dan Jurafsky and James Martin, 2024
    - <a href="https://arxiv.org/pdf/2307.06435">A Comprehensive Overview of Large Language Models</a>, Naveed et. al., 2024
